{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Technocolabs_problem-2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R98A0cvtA8n"
      },
      "source": [
        "import pandas as pd\n",
        "import tqdm\n",
        "import os\n",
        "import tensorflow as tf\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "#tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SblFJkZvtEL5",
        "outputId": "7d9a08dd-c92c-426b-f897-ee494027cdb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        }
      },
      "source": [
        "#\n",
        "#  Script variables. Change these and observe\n",
        "#  how results change.\n",
        "#\n",
        "LEARNING_RATE = 0.1\n",
        "EPOCHS = 100\n",
        "\n",
        "#\n",
        "#  Script constants. Here we pull data available\n",
        "#  locally. The data contains both the sprites\n",
        "#  and the labels from the original MNIST dataset.\n",
        "#\n",
        "LOGDIR = \"mnist_example/\"\n",
        "LABELS = os.path.join(os.getcwd(), \"/content/gdrive/My Drive/Technocolabs dataset/Dataset1/labels_1024.tsv\")\n",
        "SPRITES = os.path.join(os.getcwd(), \"/content/gdrive/My Drive/Technocolabs dataset/Dataset1/sprite_1024.png\")\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Load data form the MNIST dataset into memory.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    TensorFlow object with dataset.\n",
        "    \"\"\"\n",
        "    if not (os.path.isfile(LABELS) and os.path.isfile(SPRITES)):\n",
        "        raise ValueError(\"\"\"\n",
        "            Necessary data files were not found. Make sure the files\n",
        "\n",
        "                * labels_1024.tsv\n",
        "                * sprite_1024.png\n",
        "\n",
        "            are available in the same location where you run this script.\n",
        "            \"\"\")\n",
        "\n",
        "    return tf.contrib.learn.datasets.mnist.read_data_sets(\n",
        "        train_dir=LOGDIR + \"data\", one_hot=True)\n",
        "\n",
        "\n",
        "def convolutional_layer(input, size_in, size_out, name=\"convolutional\"):\n",
        "    \"\"\"Convoluted layer.\n",
        "\n",
        "    Create the weights and biases distributions.\n",
        "    Also define the convolution and the activation function (ReLU).\n",
        "    Finally, we create some histogram summaries useful for TensorBoard.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    size_in, size_out: int or float\n",
        "        Where to truncate the normal distribution.\n",
        "\n",
        "    name: str\n",
        "        Name to give the TensorFlow scope.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name):\n",
        "\n",
        "        W = tf.Variable(tf.truncated_normal([5, 5, size_in, size_out], stddev=0.1), name=\"Weights\")\n",
        "        B = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"Biases\")\n",
        "\n",
        "        convolution = tf.nn.conv2d(input, W, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
        "        activation = tf.nn.relu(convolution + B)\n",
        "\n",
        "        tf.summary.histogram(\"weights\", W)\n",
        "        tf.summary.histogram(\"biases\", B)\n",
        "        tf.summary.histogram(\"activations\", activation)\n",
        "\n",
        "        return tf.nn.max_pool(activation, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
        "\n",
        "\n",
        "def fully_connected_layer(input, size_in, size_out, name=\"fully_connected\"):\n",
        "    \"\"\"Fully connected layer.\n",
        "\n",
        "    This defines the fully connected layer.\n",
        "    Different from the convolution layer, this layer does not\n",
        "    perform a convolution but only defines an activation function.\n",
        "    That function is also different from the convolution layer\n",
        "    by multipliying the input data with its weights plus the biases,\n",
        "    which is a much simpler activation function than ReLU.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    size_in, size_out: int or float\n",
        "        Where to truncate the normal distribution.\n",
        "\n",
        "    name: str\n",
        "        Name to give the TensorFlow scope.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name):\n",
        "        W = tf.Variable(tf.truncated_normal([size_in, size_out], stddev=0.1), name=\"Weights\")\n",
        "        B = tf.Variable(tf.constant(0.1, shape=[size_out]), name=\"Biases\")\n",
        "\n",
        "        activation = tf.matmul(input, W) + B\n",
        "\n",
        "        tf.summary.histogram(\"weights\", W)\n",
        "        tf.summary.histogram(\"biases\", B)\n",
        "        tf.summary.histogram(\"activations\", activation)\n",
        "\n",
        "        return activation\n",
        "\n",
        "\n",
        "def model(mnist, learning_rate, epochs=2000):\n",
        "    \"\"\"Neural network model used in the MNIST dataset.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mnist: TensorFlow dataset object\n",
        "        MNIST dataset loaded using TensorFlow.\n",
        "\n",
        "    learning_rate: float\n",
        "        Learning rate at which the network should\n",
        "        create momentum.\n",
        "\n",
        "    epochs: int, default 2000\n",
        "        Number of epochs to train the model with.\n",
        "    \"\"\"\n",
        "    name = \"MNIST-model/lr={}-epochs={}\".format(learning_rate, epochs)\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    sess = tf.Session()\n",
        "\n",
        "    X = tf.placeholder(tf.float32, shape=[None, 784], name=\"X\")\n",
        "    X_image = tf.reshape(X, [-1, 28, 28, 1])\n",
        "    tf.summary.image('input', X_image, 3)\n",
        "\n",
        "    Y = tf.placeholder(tf.float32, shape=[None, 10], name=\"Labels\")\n",
        "\n",
        "    #\n",
        "    #  Convolutional layer treatment. We use a single convolutional layer.\n",
        "    #\n",
        "    convolution = convolutional_layer(X_image, 1, 64, \"Convolution_Layer\")\n",
        "    convolution_output = tf.nn.max_pool(convolution, ksize=[1, 2, 2, 1],\n",
        "                                        strides=[1, 2, 2, 1], padding=\"SAME\")\n",
        "\n",
        "    flattened = tf.reshape(convolution_output, [-1, 7 * 7 * 64])\n",
        "\n",
        "    #\n",
        "    #  Fully-connected layer treatment.\n",
        "    #  We will use two fully connected layers\n",
        "    #  that connect to each other. We can\n",
        "    #  try more or less to see the impact\n",
        "    #  on our model.\n",
        "    #\n",
        "    fully_connected_1 = fully_connected_layer(flattened, 7 * 7 * 64, 1024,\n",
        "                                              \"Fully-connected_Layer_1\")\n",
        "    relu = tf.nn.relu(fully_connected_1)\n",
        "    embedding_input = relu\n",
        "    tf.summary.histogram(\"Fully-connected_Layer-1/relu\", relu)\n",
        "\n",
        "    embedding_size = 1024\n",
        "    logits = fully_connected_layer(fully_connected_1, 1024, 10,\n",
        "                                   \"Fully-connected_Layer_2\")\n",
        "\n",
        "    with tf.name_scope(\"Cross_Entropy\"):\n",
        "        cross_entropy = tf.reduce_mean(\n",
        "            tf.nn.softmax_cross_entropy_with_logits(\n",
        "                logits=logits, labels=Y), name=\"cross_entropy\")\n",
        "\n",
        "        tf.summary.scalar(\"cross_entropy\", cross_entropy)\n",
        "\n",
        "    with tf.name_scope(\"Train\"):\n",
        "        train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
        "\n",
        "    with tf.name_scope(\"Accuracy\"):\n",
        "        correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
        "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "        tf.summary.scalar(\"Accuracy\", accuracy)\n",
        "\n",
        "    summ = tf.summary.merge_all()\n",
        "\n",
        "    #\n",
        "    #  Let's save embeddings so that they\n",
        "    #  are available in TensorBoard.\n",
        "    #\n",
        "    embedding = tf.Variable(tf.zeros([1024, embedding_size]), name=\"Test_Embedding\")\n",
        "    assignment = embedding.assign(embedding_input)\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    #\n",
        "    #  We can now run our TensorFlow session.\n",
        "    #\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    writer = tf.summary.FileWriter(LOGDIR + name)\n",
        "    writer.add_graph(sess.graph)\n",
        "\n",
        "    config = tf.contrib.tensorboard.plugins.projector.ProjectorConfig()\n",
        "    embedding_config = config.embeddings.add()\n",
        "    embedding_config.tensor_name = embedding.name\n",
        "    embedding_config.sprite.image_path = SPRITES\n",
        "    embedding_config.metadata_path = LABELS\n",
        "\n",
        "    #\n",
        "    #  Create each thumbnail.\n",
        "    #\n",
        "    embedding_config.sprite.single_image_dim.extend([28, 28])\n",
        "    tf.contrib.tensorboard.plugins.projector.visualize_embeddings(writer, config)\n",
        "\n",
        "    for i in tqdm.tqdm(range(epochs + 1), 'Epochs'):\n",
        "        batch = mnist.train.next_batch(100)\n",
        "\n",
        "        if i % 5 == 0:\n",
        "            [train_accuracy, s] = sess.run([accuracy, summ], feed_dict={X: batch[0], Y: batch[1]})\n",
        "            writer.add_summary(s, i)\n",
        "        if i % 500 == 0:\n",
        "            sess.run(assignment, feed_dict={\n",
        "                X: mnist.test.images[:1024],\n",
        "                Y: mnist.test.labels[:1024]\n",
        "            })\n",
        "            saver.save(sess, os.path.join(LOGDIR, \"model.ckpt\"), i)\n",
        "\n",
        "        sess.run(train_step, feed_dict={X: batch[0], Y: batch[1]})\n",
        "\n",
        "    print('Done training!')\n",
        "    print('Run `tensorboard --logdir=%s` to see the results.' % LOGDIR)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main runner function.\n",
        "\n",
        "    This calls the model function.\n",
        "    \"\"\"\n",
        "    print(\"\"\"\n",
        "    Running MNIST model with:\n",
        "\n",
        "        * Learning rate: {}\n",
        "        * Epochs: {}\n",
        "\n",
        "    \"\"\".format(LEARNING_RATE, EPOCHS))\n",
        "\n",
        "    mnist = load_data()\n",
        "    model(mnist=mnist, learning_rate=LEARNING_RATE, epochs=EPOCHS)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "    Running MNIST model with:\n",
            "\n",
            "        * Learning rate: 0.0001\n",
            "        * Epochs: 2000\n",
            "\n",
            "    \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-ba670a7096e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-ba670a7096e4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m     \"\"\".format(LEARNING_RATE, EPOCHS))\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mmnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-ba670a7096e4>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \"\"\")\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     return tf.contrib.learn.datasets.mnist.read_data_sets(\n\u001b[0m\u001b[1;32m     36\u001b[0m         train_dir=LOGDIR + \"data\", one_hot=True)\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnNCYW59XYIZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or6TfbzNpXGB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}